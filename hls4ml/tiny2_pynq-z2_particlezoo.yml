save_dir: resnet_v1_eembc_quantized_particlezoo_quantized_tiny2
data:
  name: particlezoo
  input_shape:
  - 32
  - 32
  - 3
  num_classes: 36
model:
  name: resnet_v1_eembc_quantized
  filters:
  - 32
  - 4
  - 32
  - 32
  l1: 0
  l2: 1e-4
  kernels:
  - 1
  - 4
  - 4
  - 4
  - 4
  - 4
  strides:
  - '111'
  - '414'
  skip: 0
  avg_pooling: 0
  final_activation: 1

pruning:
  sparsity: 1.0

fit:
  compile:
    initial_lr: 0.001
    lr_decay: 0.99
    optimizer: Adam
    loss: categorical_crossentropy
  epochs: 1000
  patience: 1000
  batch_size: 36
  verbose: 1

quantization:
  logit_total_bits: 8
  logit_int_bits: 2
  logit_quantizer: quantized_bits
  activation_total_bits: 8
  activation_int_bits: 2
  activation_quantizer: quantized_relu
  alpha: 1
  use_stochastic_rounding: 0

convert:
  ApplyPatches: 1
  RemoveSoftmax: 1
  OutputDir: my-hls-test-quantized-tiny2-pynq-z2-rf16384-6p67ns_FIFO_OPT
  XilinxPart: xc7z020clg400-1
  Backend: VivadoAccelerator
  IOType: io_stream
  Interface: axi_master
  Driver: c
  InputType: ap_uint<8>
  OutputType: float
  Board: pynq-z2
  Precision: ap_fixed<12,7>
  ReuseFactor: 16384
  Trace: 0
  Build: 1
  EEMBC_power: 0
  FIFO_opt: 0
  FIFO_opt_json: max_depth_rf16384.json
  MergedRelu: 1
  ClockPeriod: 6.67
  Strategy: Resource
  Override:
    input_1:
      Precision: ap_ufixed<8,0>
    q_conv2d_batchnorm:
      accum_t: ap_fixed<17,7>
      Precision:
        weight: ap_fixed<8,3>
        bias: ap_fixed<8,3>
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
      ConvImplementation: 'LineBuffer'
    q_conv2d_batchnorm_linear:
      Precision:
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
    q_activation:
      Precision:
        default: ap_fixed<12,7>
        result: ap_fixed<9,3,AP_RND,AP_SAT>
    q_conv2d_batchnorm_1:
      accum_t: ap_fixed<17,7>
      Precision:
        weight: ap_fixed<8,3>
        bias: ap_fixed<8,3>
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
      ConvImplementation: 'LineBuffer'
    q_conv2d_batchnorm_1_linear:
      Precision:
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
    q_activation_1:
      Precision:
        default: ap_fixed<12,7>
        result: ap_fixed<9,3,AP_RND,AP_SAT>
    q_conv2d_batchnorm_2:
      accum_t: ap_fixed<17,7>
      Precision:
        weight: ap_fixed<8,3>
        bias: ap_fixed<8,3>
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
      ConvImplementation: 'LineBuffer'
    q_conv2d_batchnorm_2_linear:
      Precision:
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
    q_activation_2:
      Precision:
        default: ap_fixed<12,7>
        result: ap_fixed<9,3,AP_RND,AP_SAT>
    q_conv2d_batchnorm_3:
      accum_t: ap_fixed<17,7>
      Precision:
        weight: ap_fixed<8,3>
        bias: ap_fixed<8,3>
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
      ConvImplementation: 'LineBuffer'
    q_conv2d_batchnorm_3_linear:
      Precision:
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
    q_activation_3:
      Precision:
        default: ap_fixed<12,7>
        result: ap_fixed<9,3,AP_RND,AP_SAT>
    q_conv2d_batchnorm_4:
      accum_t: ap_fixed<17,7>
      Precision:
        weight: ap_fixed<8,3>
        bias: ap_fixed<8,3>
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
      ConvImplementation: 'LineBuffer'
    q_conv2d_batchnorm_4_linear:
      Precision:
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
    q_activation_5:
      Precision:
        default: ap_fixed<12,7>
        result: ap_fixed<9,3,AP_RND,AP_SAT>
    q_dense:
      accum_t: ap_fixed<17,7>
      Precision:
        weight: ap_fixed<8,3>
        bias: ap_fixed<8,3>
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
    q_dense_linear:
      Precision:
        result: ap_fixed<12,7>
        default: ap_fixed<12,7>
